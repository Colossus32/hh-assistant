# Настройка Ollama (локальный запуск)

Ollama запускается локально на вашей машине, а не в Docker контейнере. Это упрощает настройку и позволяет использовать GPU напрямую.

## Установка Ollama на Windows

1. **Скачайте установщик:**
   - Перейдите на https://ollama.ai/download
   - Скачайте установщик для Windows
   - Запустите установщик и следуйте инструкциям

2. **Проверка установки:**
   ```powershell
   ollama --version
   ```

## Запуск Ollama

Ollama запускается как сервис автоматически после установки. Если нужно запустить вручную:

```powershell
ollama serve
```

Сервис будет доступен на `http://localhost:11434`

## Загрузка модели Qwen 2.5

```powershell
ollama pull qwen2.5:7b
```

Это загрузит модель ~4GB. Процесс может занять время в зависимости от скорости интернета.

## Проверка работы

1. **Проверка доступности API:**
   ```powershell
   curl http://localhost:11434/api/tags
   ```

2. **Тестовый запрос:**
   ```powershell
   ollama run qwen2.5:7b "Привет! Как дела?"
   ```

## Альтернативные модели (если Qwen 2.5 не подходит)

Для 6GB VRAM можно использовать:
- `llama3.2:3b` - быстрая, легкая модель
- `mistral:7b` - хорошая альтернатива (quantized)
- `phi3:3.8b` - от Microsoft

Загрузка:
```powershell
ollama pull llama3.2:3b
```

## Настройка для Docker приложения

Если приложение запускается в Docker, но Ollama локально:

1. **В docker-compose.yml** уже настроено:
   ```yaml
   OLLAMA_BASE_URL: http://host.docker.internal:11434
   ```

2. **Для локального запуска приложения** (без Docker):
   ```yaml
   OLLAMA_BASE_URL: http://localhost:11434
   ```

## Troubleshooting

**Проблема: Приложение не может подключиться к Ollama**

- Убедитесь, что Ollama запущен: `ollama serve`
- Проверьте, что порт 11434 не заблокирован файрволом
- Для Docker: используйте `host.docker.internal:11434` вместо `localhost:11434`

**Проблема: Модель не загружается**

- Проверьте свободное место на диске (нужно ~4-5GB для Qwen 2.5:7b)
- Проверьте интернет-соединение
- Попробуйте другую модель (например, llama3.2:3b)

**Проблема: Медленная работа**

- Используйте GPU (если доступен) - Ollama автоматически использует GPU если драйверы установлены
- Попробуйте более легкую модель (llama3.2:3b вместо qwen2.5:7b)
- Увеличьте timeout в application.yml


