# Apache Kafka –¥–ª—è —Å–æ–±–µ—Å–µ–¥–æ–≤–∞–Ω–∏–π

**Java/Kotlin Backend Developer | Middle/Senior**

## Producer

### –ö–ï–ô–° #1 | –£—Ä–æ–≤–µ–Ω—å: Middle
**–í–û–ü–†–û–°:** –ß—Ç–æ —Ç–∞–∫–æ–µ acks –≤ Kafka Producer? –†–∞–∑–Ω–∏—Ü–∞ –º–µ–∂–¥—É acks=0, 1, all?

**–û–¢–í–ï–¢:**
```kotlin
// acks=0: –Ω–µ –∂–¥—ë–º –ø–æ–¥—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏—è (fastest, –Ω–æ –º–æ–∂–µ—Ç –ø–æ—Ç–µ—Ä—è—Ç—å –¥–∞–Ω–Ω—ã–µ)
val props = Properties().apply {
    put(ProducerConfig.ACKS_CONFIG, "0")
    put(ProducerConfig.RETRIES_CONFIG, 0)
}
// –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –¥–ª—è –º–µ—Ç—Ä–∏–∫, –ª–æ–≥–æ–≤ (–¥–æ–ø—É—Å—Ç–∏–º–∞ –ø–æ—Ç–µ—Ä—è)

// acks=1: –∂–¥—ë–º –ø–æ–¥—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏—è –æ—Ç leader (–±–∞–ª–∞–Ω—Å)
put(ProducerConfig.ACKS_CONFIG, "1")
// Leader –∑–∞–ø–∏—Å–∞–ª, –Ω–æ —Ä–µ–ø–ª–∏–∫–∏ –º–æ–≥—É—Ç –Ω–µ —É—Å–ø–µ—Ç—å ‚Üí —Ä–∏—Å–∫ –ø–æ—Ç–µ—Ä–∏

// acks=all (–∏–ª–∏ -1): –∂–¥—ë–º –æ—Ç leader + –≤—Å–µ in-sync —Ä–µ–ø–ª–∏–∫–∏ (safest)
put(ProducerConfig.ACKS_CONFIG, "all")
put(ProducerConfig.MIN_IN_SYNC_REPLICAS_CONFIG, 2)  // –ú–∏–Ω–∏–º—É–º 2 —Ä–µ–ø–ª–∏–∫–∏
// –ì–∞—Ä–∞–Ω—Ç–∏—è: –¥–∞–Ω–Ω—ã–µ –Ω–∞ >=2 –±—Ä–æ–∫–µ—Ä–∞—Ö

// –ò–¥–µ–º–ø–æ—Ç–µ–Ω—Ç–Ω–æ—Å—Ç—å (exactly-once)
put(ProducerConfig.ENABLE_IDEMPOTENCE_CONFIG, true)
// –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏: acks=all, retries=MAX_INT, max.in.flight=5

// –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ
val producer = KafkaProducer<String, String>(props)

val record = ProducerRecord("orders", orderId.toString(), orderJson)
producer.send(record) { metadata, exception ->
    if (exception != null) {
        logger.error("Failed to send", exception)
    } else {
        logger.info("Sent to partition ${metadata.partition()}, offset ${metadata.offset()}")
    }
}
```

## Consumer

### –ö–ï–ô–° #5 | –£—Ä–æ–≤–µ–Ω—å: Senior
**–í–û–ü–†–û–°:** –ö–∞–∫ —Ä–∞–±–æ—Ç–∞–µ—Ç Consumer Group? –ß—Ç–æ –ø—Ä–æ–∏—Å—Ö–æ–¥–∏—Ç –ø—Ä–∏ rebalancing?

**–û–¢–í–ï–¢:**
```kotlin
// Consumer Group: –Ω–µ—Å–∫–æ–ª—å–∫–æ consumers —á–∏—Ç–∞—é—Ç –∏–∑ –æ–¥–Ω–æ–≥–æ —Ç–æ–ø–∏–∫–∞
// –ö–∞–∂–¥–∞—è –ø–∞—Ä—Ç–∏—Ü–∏—è —á–∏—Ç–∞–µ—Ç—Å—è —Ç–æ–ª—å–∫–æ 1 consumer'–æ–º –∏–∑ –≥—Ä—É–ø–ø—ã

// –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è
val props = Properties().apply {
    put(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG, "localhost:9092")
    put(ConsumerConfig.GROUP_ID_CONFIG, "order-processing-group")
    put(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, StringDeserializer::class.java)
    put(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, StringDeserializer::class.java)
    
    // Offset commit strategy
    put(ConsumerConfig.ENABLE_AUTO_COMMIT_CONFIG, false)  // –†—É—á–Ω–æ–π commit
    put(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG, "earliest")
}

val consumer = KafkaConsumer<String, String>(props)
consumer.subscribe(listOf("orders"))

// –ü–æ—Ç—Ä–µ–±–ª–µ–Ω–∏–µ
while (true) {
    val records = consumer.poll(Duration.ofSeconds(1))
    
    records.forEach { record ->
        try {
            processOrder(record.value())
            
            // Commit –ü–û–°–õ–ï —É—Å–ø–µ—à–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏
            consumer.commitSync(
                mapOf(
                    TopicPartition(record.topic(), record.partition()) 
                        to OffsetAndMetadata(record.offset() + 1)
                )
            )
        } catch (e: Exception) {
            logger.error("Failed to process record", e)
            // –ù–µ –∫–æ–º–º–∏—Ç–∏–º ‚Äî –ø–æ–≤—Ç–æ—Ä–∏–º –ø—Ä–∏ —Å–ª–µ–¥—É—é—â–µ–º poll()
        }
    }
}

// REBALANCING: –ø–µ—Ä–µ—Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø–∞—Ä—Ç–∏—Ü–∏–π
// –ü—Ä–æ–∏—Å—Ö–æ–¥–∏—Ç –∫–æ–≥–¥–∞:
// 1. –î–æ–±–∞–≤–ª—è–µ—Ç—Å—è/—É–¥–∞–ª—è–µ—Ç—Å—è consumer
// 2. Consumer –ø–∞–¥–∞–µ—Ç (heartbeat timeout)
// 3. –î–æ–±–∞–≤–ª—è—é—Ç—Å—è –Ω–æ–≤—ã–µ –ø–∞—Ä—Ç–∏—Ü–∏–∏

// –û–±—Ä–∞–±–æ—Ç–∫–∞ rebalancing
consumer.subscribe(
    listOf("orders"),
    object : ConsumerRebalanceListener {
        override fun onPartitionsRevoked(partitions: Collection<TopicPartition>) {
            // –í—ã–∑—ã–≤–∞–µ—Ç—Å—è –ü–ï–†–ï–î rebalancing
            logger.warn("Partitions revoked: $partitions")
            consumer.commitSync()  // –ö–æ–º–º–∏—Ç–∏–º –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–µ offset'—ã
        }
        
        override fun onPartitionsAssigned(partitions: Collection<TopicPartition>) {
            // –í—ã–∑—ã–≤–∞–µ—Ç—Å—è –ü–û–°–õ–ï rebalancing
            logger.info("Partitions assigned: $partitions")
        }
    }
)
```

### –ö–ï–ô–° #2 | –£—Ä–æ–≤–µ–Ω—å: Middle
**–í–û–ü–†–û–°:** –ö–∞–∫ –ø—Ä–∞–≤–∏–ª—å–Ω–æ –≤—ã–±—Ä–∞—Ç—å –∫–ª—é—á –ø–∞—Ä—Ç–∏—Ü–∏–æ–Ω–∏—Ä–æ–≤–∞–Ω–∏—è –¥–ª—è Kafka Producer?

**–û–¢–í–ï–¢:**
```kotlin
// –ü–∞—Ä—Ç–∏—Ü–∏–æ–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ –æ–ø—Ä–µ–¥–µ–ª—è–µ—Ç, –≤ –∫–∞–∫—É—é –ø–∞—Ä—Ç–∏—Ü–∏—é –ø–æ–ø–∞–¥—ë—Ç —Å–æ–æ–±—â–µ–Ω–∏–µ
// –°–æ–æ–±—â–µ–Ω–∏—è —Å –æ–¥–Ω–∏–º –∫–ª—é—á–æ–º ‚Üí –≤—Å–µ–≥–¥–∞ –≤ –æ–¥–Ω—É –ø–∞—Ä—Ç–∏—Ü–∏—é ‚Üí –ø–æ—Ä—è–¥–æ–∫ –≥–∞—Ä–∞–Ω—Ç–∏—Ä–æ–≤–∞–Ω

// ‚ùå –ü–õ–û–•–û: –±–µ–∑ –∫–ª—é—á–∞ (round-robin)
producer.send(ProducerRecord("orders", null, orderJson))
// –°–æ–æ–±—â–µ–Ω–∏—è –æ–¥–Ω–æ–≥–æ –∑–∞–∫–∞–∑–∞ –º–æ–≥—É—Ç –ø–æ–ø–∞—Å—Ç—å –≤ —Ä–∞–∑–Ω—ã–µ –ø–∞—Ä—Ç–∏—Ü–∏–∏ ‚Üí –Ω–∞—Ä—É—à–∏—Ç—Å—è –ø–æ—Ä—è–¥–æ–∫

// ‚úÖ –•–û–†–û–®–û: –∫–ª—é—á = orderId
producer.send(ProducerRecord("orders", orderId.toString(), orderJson))
// –í—Å–µ —Å–æ–±—ã—Ç–∏—è –æ–¥–Ω–æ–≥–æ –∑–∞–∫–∞–∑–∞ ‚Üí –≤ –æ–¥–Ω—É –ø–∞—Ä—Ç–∏—Ü–∏—é ‚Üí –ø–æ—Ä—è–¥–æ–∫ —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç—Å—è

// –ü—Ä–∏–º–µ—Ä: –æ–±—Ä–∞–±–æ—Ç–∫–∞ –∑–∞–∫–∞–∑–∞
@Service
class OrderEventProducer(private val kafkaTemplate: KafkaTemplate<String, OrderEvent>) {
    
    fun sendOrderEvent(orderId: String, event: OrderEvent) {
        // orderId –∫–∞–∫ –∫–ª—é—á ‚Äî –≤—Å–µ —Å–æ–±—ã—Ç–∏—è –∑–∞–∫–∞–∑–∞ –≤ –æ–¥–Ω–æ–π –ø–∞—Ä—Ç–∏—Ü–∏–∏
        kafkaTemplate.send("order-events", orderId, event)
            .whenComplete { result, ex ->
                if (ex == null) {
                    logger.info("Event sent to partition ${result.recordMetadata.partition()}")
                } else {
                    logger.error("Failed to send event", ex)
                }
            }
    }
}

// –ö–∞—Å—Ç–æ–º–Ω—ã–π –ø–∞—Ä—Ç–∏—Ü–∏–æ–Ω–µ—Ä
class CustomerPartitioner : Partitioner {
    override fun partition(
        topic: String,
        key: Any?,
        keyBytes: ByteArray?,
        value: Any?,
        valueBytes: ByteArray?,
        cluster: Cluster
    ): Int {
        val customerId = (key as String).substringBefore("-")
        val partitions = cluster.partitionsForTopic(topic).size
        
        // VIP –∫–ª–∏–µ–Ω—Ç—ã ‚Üí –≤—Å–µ–≥–¥–∞ –≤ –ø–∞—Ä—Ç–∏—Ü–∏—é 0 (–±—ã—Å—Ç—Ä–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞)
        return if (isVipCustomer(customerId)) 0 
               else abs(customerId.hashCode()) % (partitions - 1) + 1
    }
}
```

### –ö–ï–ô–° #3 | –£—Ä–æ–≤–µ–Ω—å: Senior
**–í–û–ü–†–û–°:** –ß—Ç–æ —Ç–∞–∫–æ–µ –∏–¥–µ–º–ø–æ—Ç–µ–Ω—Ç–Ω–æ—Å—Ç—å –≤ Kafka Producer? –ö–∞–∫ –æ–Ω–∞ —Ä–∞–±–æ—Ç–∞–µ—Ç?

**–û–¢–í–ï–¢:**
```kotlin
// –ò–¥–µ–º–ø–æ—Ç–µ–Ω—Ç–Ω–æ—Å—Ç—å: –ø–æ–≤—Ç–æ—Ä–Ω–∞—è –æ—Ç–ø—Ä–∞–≤–∫–∞ –Ω–µ —Å–æ–∑–¥–∞—Å—Ç –¥—É–±–ª–∏–∫–∞—Ç—ã
// Producer –Ω–∞–∑–Ω–∞—á–∞–µ—Ç –∫–∞–∂–¥–æ–º—É —Å–æ–æ–±—â–µ–Ω–∏—é sequence number

// –í–∫–ª—é—á–µ–Ω–∏–µ –∏–¥–µ–º–ø–æ—Ç–µ–Ω—Ç–Ω–æ—Å—Ç–∏
val props = Properties().apply {
    put(ProducerConfig.ENABLE_IDEMPOTENCE_CONFIG, true)
    // –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ —É—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ—Ç—Å—è:
    // - acks=all
    // - retries=Integer.MAX_VALUE
    // - max.in.flight.requests.per.connection=5
}

// –ö–∞–∫ —ç—Ç–æ —Ä–∞–±–æ—Ç–∞–µ—Ç:
// Producer –≥–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç PID (Producer ID) –∏ sequence number –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —Å–æ–æ–±—â–µ–Ω–∏—è
// Broker –æ—Ç–∫–ª–æ–Ω—è–µ—Ç –¥—É–±–ª–∏–∫–∞—Ç—ã (—Å —Ç–µ–º –∂–µ PID –∏ sequence number)

@Configuration
class KafkaProducerConfig {
    
    @Bean
    fun producerFactory(): ProducerFactory<String, OrderEvent> {
        val props = mapOf(
            ProducerConfig.BOOTSTRAP_SERVERS_CONFIG to "localhost:9092",
            ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG to StringSerializer::class.java,
            ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG to JsonSerializer::class.java,
            
            // –ò–¥–µ–º–ø–æ—Ç–µ–Ω—Ç–Ω–æ—Å—Ç—å + —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏–∏ = exactly-once
            ProducerConfig.ENABLE_IDEMPOTENCE_CONFIG to true,
            ProducerConfig.TRANSACTIONAL_ID_CONFIG to "order-producer-1"
        )
        return DefaultKafkaProducerFactory(props)
    }
}

// –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ —Å —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏—è–º–∏
@Service
class TransactionalOrderProducer(
    private val kafkaTemplate: KafkaTemplate<String, OrderEvent>
) {
    
    @Transactional("kafkaTransactionManager")
    fun processOrder(order: Order) {
        // –í—Å–µ send() –≤ —Ä–∞–º–∫–∞—Ö –æ–¥–Ω–æ–π —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏–∏
        kafkaTemplate.send("orders", order.id.toString(), OrderCreatedEvent(order))
        kafkaTemplate.send("inventory", order.productId.toString(), ReserveStockEvent(order))
        
        // –ï—Å–ª–∏ —É–ø–∞–¥—ë—Ç ‚Äî –æ—Ç–∫–∞—Ç—è—Ç—Å—è –û–ë–ê —Å–æ–æ–±—â–µ–Ω–∏—è
        if (order.amount > 10000) {
            kafkaTemplate.send("alerts", "large-order", LargeOrderAlert(order))
        }
    }
}
```

### –ö–ï–ô–° #4 | –£—Ä–æ–≤–µ–Ω—å: Middle
**–í–û–ü–†–û–°:** –ß—Ç–æ —Ç–∞–∫–æ–µ max.in.flight.requests.per.connection? –ö–∞–∫ —ç—Ç–æ –≤–ª–∏—è–µ—Ç –Ω–∞ –ø–æ—Ä—è–¥–æ–∫ —Å–æ–æ–±—â–µ–Ω–∏–π?

**–û–¢–í–ï–¢:**
```kotlin
// max.in.flight.requests.per.connection ‚Äî –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –Ω–µ–ø–æ–¥—Ç–≤–µ—Ä–∂–¥—ë–Ω–Ω—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤
// –∫–æ—Ç–æ—Ä—ã–µ Producer –º–æ–∂–µ—Ç –æ—Ç–ø—Ä–∞–≤–∏—Ç—å –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω–æ

// ‚ùå –ü–†–û–ë–õ–ï–ú–ê: –Ω–∞—Ä—É—à–µ–Ω–∏–µ –ø–æ—Ä—è–¥–∫–∞ –ø—Ä–∏ max.in.flight > 1 + retries > 0
// Batch 1: [msg1, msg2] ‚Äî –æ—Ç–ø—Ä–∞–≤–ª–µ–Ω
// Batch 2: [msg3, msg4] ‚Äî –æ—Ç–ø—Ä–∞–≤–ª–µ–Ω
// Batch 1 —É–ø–∞–ª ‚Üí retry
// Batch 2 —É—Å–ø–µ—à–Ω–æ –∑–∞–ø–∏—Å–∞–Ω
// Batch 1 retry —É—Å–ø–µ—à–µ–Ω
// –ü–æ—Ä—è–¥–æ–∫ –≤ —Ç–æ–ø–∏–∫–µ: msg3, msg4, msg1, msg2 ‚ùå

// ‚úÖ –†–ï–®–ï–ù–ò–ï 1: max.in.flight = 1 (–º–µ–¥–ª–µ–Ω–Ω–æ)
val props = Properties().apply {
    put(ProducerConfig.MAX_IN_FLIGHT_REQUESTS_PER_CONNECTION, 1)
    put(ProducerConfig.RETRIES_CONFIG, Int.MAX_VALUE)
}
// –ü–æ—Ä—è–¥–æ–∫ –≥–∞—Ä–∞–Ω—Ç–∏—Ä–æ–≤–∞–Ω, –Ω–æ throughput —Å—Ç—Ä–∞–¥–∞–µ—Ç

// ‚úÖ –†–ï–®–ï–ù–ò–ï 2: –∏–¥–µ–º–ø–æ—Ç–µ–Ω—Ç–Ω–æ—Å—Ç—å (—Ä–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è)
put(ProducerConfig.ENABLE_IDEMPOTENCE_CONFIG, true)
put(ProducerConfig.MAX_IN_FLIGHT_REQUESTS_PER_CONNECTION, 5)  // –î–æ 5 —Ä–∞–∑—Ä–µ—à–µ–Ω–æ
// Broker –≥–∞—Ä–∞–Ω—Ç–∏—Ä—É–µ—Ç –ø–æ—Ä—è–¥–æ–∫ –∑–∞ —Å—á—ë—Ç sequence numbers

@Configuration
class OrderedKafkaProducerConfig {
    
    @Bean
    fun producerFactory(): ProducerFactory<String, String> {
        return DefaultKafkaProducerFactory(mapOf(
            ProducerConfig.BOOTSTRAP_SERVERS_CONFIG to "localhost:9092",
            ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG to StringSerializer::class.java,
            ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG to StringSerializer::class.java,
            
            // –ì–∞—Ä–∞–Ω—Ç–∏—è –ø–æ—Ä—è–¥–∫–∞ + –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å
            ProducerConfig.ENABLE_IDEMPOTENCE_CONFIG to true,
            ProducerConfig.MAX_IN_FLIGHT_REQUESTS_PER_CONNECTION to 5,
            ProducerConfig.ACKS_CONFIG to "all",
            ProducerConfig.RETRIES_CONFIG to Int.MAX_VALUE
        ))
    }
}
```

## Consumer

### –ö–ï–ô–° #5 | –£—Ä–æ–≤–µ–Ω—å: Senior
**–í–û–ü–†–û–°:** –ö–∞–∫ —Ä–∞–±–æ—Ç–∞–µ—Ç Consumer Group? –ß—Ç–æ –ø—Ä–æ–∏—Å—Ö–æ–¥–∏—Ç –ø—Ä–∏ rebalancing?

**–û–¢–í–ï–¢:**
```kotlin
// Consumer Group: –Ω–µ—Å–∫–æ–ª—å–∫–æ consumers —á–∏—Ç–∞—é—Ç –∏–∑ –æ–¥–Ω–æ–≥–æ —Ç–æ–ø–∏–∫–∞
// –ö–∞–∂–¥–∞—è –ø–∞—Ä—Ç–∏—Ü–∏—è —á–∏—Ç–∞–µ—Ç—Å—è —Ç–æ–ª—å–∫–æ 1 consumer'–æ–º –∏–∑ –≥—Ä—É–ø–ø—ã

// –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è
val props = Properties().apply {
    put(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG, "localhost:9092")
    put(ConsumerConfig.GROUP_ID_CONFIG, "order-processing-group")
    put(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, StringDeserializer::class.java)
    put(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, StringDeserializer::class.java)
    
    // Offset commit strategy
    put(ConsumerConfig.ENABLE_AUTO_COMMIT_CONFIG, false)  // –†—É—á–Ω–æ–π commit
    put(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG, "earliest")
}

val consumer = KafkaConsumer<String, String>(props)
consumer.subscribe(listOf("orders"))

// –ü–æ—Ç—Ä–µ–±–ª–µ–Ω–∏–µ
while (true) {
    val records = consumer.poll(Duration.ofSeconds(1))
    
    records.forEach { record ->
        try {
            processOrder(record.value())
            
            // Commit –ü–û–°–õ–ï —É—Å–ø–µ—à–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏
            consumer.commitSync(
                mapOf(
                    TopicPartition(record.topic(), record.partition()) 
                        to OffsetAndMetadata(record.offset() + 1)
                )
            )
        } catch (e: Exception) {
            logger.error("Failed to process record", e)
            // –ù–µ –∫–æ–º–º–∏—Ç–∏–º ‚Äî –ø–æ–≤—Ç–æ—Ä–∏–º –ø—Ä–∏ —Å–ª–µ–¥—É—é—â–µ–º poll()
        }
    }
}

// REBALANCING: –ø–µ—Ä–µ—Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø–∞—Ä—Ç–∏—Ü–∏–π
// –ü—Ä–æ–∏—Å—Ö–æ–¥–∏—Ç –∫–æ–≥–¥–∞:
// 1. –î–æ–±–∞–≤–ª—è–µ—Ç—Å—è/—É–¥–∞–ª—è–µ—Ç—Å—è consumer
// 2. Consumer –ø–∞–¥–∞–µ—Ç (heartbeat timeout)
// 3. –î–æ–±–∞–≤–ª—è—é—Ç—Å—è –Ω–æ–≤—ã–µ –ø–∞—Ä—Ç–∏—Ü–∏–∏

// –û–±—Ä–∞–±–æ—Ç–∫–∞ rebalancing
consumer.subscribe(
    listOf("orders"),
    object : ConsumerRebalanceListener {
        override fun onPartitionsRevoked(partitions: Collection<TopicPartition>) {
            // –í—ã–∑—ã–≤–∞–µ—Ç—Å—è –ü–ï–†–ï–î rebalancing
            logger.warn("Partitions revoked: $partitions")
            consumer.commitSync()  // –ö–æ–º–º–∏—Ç–∏–º –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–µ offset'—ã
        }
        
        override fun onPartitionsAssigned(partitions: Collection<TopicPartition>) {
            // –í—ã–∑—ã–≤–∞–µ—Ç—Å—è –ü–û–°–õ–ï rebalancing
            logger.info("Partitions assigned: $partitions")
        }
    }
)
```

### –ö–ï–ô–° #6 | –£—Ä–æ–≤–µ–Ω—å: Middle
**–í–û–ü–†–û–°:** –í —á—ë–º —Ä–∞–∑–Ω–∏—Ü–∞ –º–µ–∂–¥—É commitSync() –∏ commitAsync()? –ö–æ–≥–¥–∞ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –∫–∞–∂–¥—ã–π?

**–û–¢–í–ï–¢:**
```kotlin
// commitSync(): –±–ª–æ–∫–∏—Ä—É–µ—Ç –ø–æ—Ç–æ–∫, –∂–¥—ë—Ç –ø–æ–¥—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏—è –æ—Ç –±—Ä–æ–∫–µ—Ä–∞
@KafkaListener(topics = ["orders"], groupId = "order-processing")
fun consumeOrderSync(message: OrderEvent) {
    processOrder(message)
    
    // ‚ùå –ü–†–û–ë–õ–ï–ú–ê: –±–ª–æ–∫–∏—Ä—É–µ—Ç Consumer loop
    // Throughput —Å—Ç—Ä–∞–¥–∞–µ—Ç, –Ω–æ –≥–∞—Ä–∞–Ω—Ç–∏—è —á—Ç–æ offset –∑–∞–ø–∏—Å–∞–Ω
}

// Spring Kafka –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é –∏—Å–ø–æ–ª—å–∑—É–µ—Ç batch commit –ø–æ—Å–ª–µ –∫–∞–∂–¥–æ–≥–æ poll()

// commitAsync(): –Ω–µ –±–ª–æ–∫–∏—Ä—É–µ—Ç, callback –ø—Ä–∏ –∑–∞–≤–µ—Ä—à–µ–Ω–∏–∏
val consumer = KafkaConsumer<String, String>(props)
while (true) {
    val records = consumer.poll(Duration.ofMillis(100))
    
    records.forEach { record ->
        processOrder(record.value())
    }
    
    // ‚úÖ –ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω—ã–π commit (–Ω–µ –±–ª–æ–∫–∏—Ä—É–µ—Ç)
    consumer.commitAsync { offsets, exception ->
        if (exception != null) {
            logger.error("Commit failed for offsets: $offsets", exception)
            // –ú–æ–∂–Ω–æ retry –∏–ª–∏ fallback –Ω–∞ commitSync()
        } else {
            logger.debug("Committed offsets: $offsets")
        }
    }
}

// ‚úÖ BEST PRACTICE: commitAsync() –≤ loop + commitSync() –ø—Ä–∏ shutdown
class OrderConsumer : AutoCloseable {
    private val consumer = KafkaConsumer<String, String>(props)
    @Volatile private var running = true
    
    fun start() {
        consumer.subscribe(listOf("orders"))
        
        try {
            while (running) {
                val records = consumer.poll(Duration.ofMillis(100))
                processRecords(records)
                
                // Async commit –¥–ª—è –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
                consumer.commitAsync()
            }
        } finally {
            // Sync commit –ø—Ä–∏ shutdown ‚Äî –≥–∞—Ä–∞–Ω—Ç–∏—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è offset
            consumer.commitSync()
            consumer.close()
        }
    }
    
    override fun close() {
        running = false
    }
}

// Spring Kafka –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è
@Configuration
class KafkaConsumerConfig {
    
    @Bean
    fun consumerFactory(): ConsumerFactory<String, OrderEvent> {
        return DefaultKafkaConsumerFactory(mapOf(
            ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG to "localhost:9092",
            ConsumerConfig.GROUP_ID_CONFIG to "order-processing",
            ConsumerConfig.ENABLE_AUTO_COMMIT_CONFIG to false,  // –†—É—á–Ω–æ–π commit
            ConsumerConfig.AUTO_OFFSET_RESET_CONFIG to "earliest"
        ))
    }
    
    @Bean
    fun kafkaListenerContainerFactory(): ConcurrentKafkaListenerContainerFactory<String, OrderEvent> {
        val factory = ConcurrentKafkaListenerContainerFactory<String, OrderEvent>()
        factory.consumerFactory = consumerFactory()
        
        // AckMode.MANUAL: —Ä—É—á–Ω–æ–π –∫–æ–Ω—Ç—Ä–æ–ª—å commit
        factory.containerProperties.ackMode = ContainerProperties.AckMode.MANUAL
        
        return factory
    }
}

@Service
class ManualCommitConsumer {
    
    @KafkaListener(topics = ["orders"])
    fun consume(message: OrderEvent, acknowledgment: Acknowledgment) {
        try {
            processOrder(message)
            acknowledgment.acknowledge()  // Async commit
        } catch (e: Exception) {
            // –ù–µ ack ‚Äî –ø–æ–≤—Ç–æ—Ä–∏–º –ø—Ä–∏ —Å–ª–µ–¥—É—é—â–µ–º poll()
            logger.error("Failed to process", e)
        }
    }
}
```

### –ö–ï–ô–° #7 | –£—Ä–æ–≤–µ–Ω—å: Senior
**–í–û–ü–†–û–°:** –ö–∞–∫ –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞—Ç—å backpressure –≤ Kafka Consumer?

**–û–¢–í–ï–¢:**
```kotlin
// Backpressure: Consumer –Ω–µ —É—Å–ø–µ–≤–∞–µ—Ç –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞—Ç—å —Å–æ–æ–±—â–µ–Ω–∏—è
// –†–µ—à–µ–Ω–∏—è:

// 1. –û–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ max.poll.records
@Configuration
class BackpressureConfig {
    
    @Bean
    fun consumerFactory(): ConsumerFactory<String, OrderEvent> {
        return DefaultKafkaConsumerFactory(mapOf(
            ConsumerConfig.MAX_POLL_RECORDS_CONFIG to 10,  // –ù–µ –±–æ–ª–µ–µ 10 –∑–∞ —Ä–∞–∑
            ConsumerConfig.MAX_POLL_INTERVAL_MS_CONFIG to 300_000  // 5 –º–∏–Ω—É—Ç –Ω–∞ –æ–±—Ä–∞–±–æ—Ç–∫—É
        ))
    }
}

// 2. Pause/Resume –ø–∞—Ä—Ç–∏—Ü–∏–π
@Service
class AdaptiveConsumer {
    
    @KafkaListener(topics = ["orders"])
    fun consume(
        message: OrderEvent,
        @Header(KafkaHeaders.RECEIVED_PARTITION_ID) partition: Int,
        consumer: Consumer<*, *>
    ) {
        // –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–≥—Ä—É–∑–∫—É
        val queueSize = processingQueue.size()
        
        if (queueSize > 1000) {
            // –ü—Ä–∏–æ—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º —á—Ç–µ–Ω–∏–µ –∏–∑ –ø–∞—Ä—Ç–∏—Ü–∏–∏
            val topicPartition = TopicPartition("orders", partition)
            consumer.pause(listOf(topicPartition))
            logger.warn("Paused partition $partition due to backpressure")
            
            // –ß–µ—Ä–µ–∑ –Ω–µ–∫–æ—Ç–æ—Ä–æ–µ –≤—Ä–µ–º—è –≤–æ–∑–æ–±–Ω–æ–≤–ª—è–µ–º
            scheduleResume(consumer, topicPartition)
        }
        
        processingQueue.add(message)
    }
}

// 3. Thread pool –¥–ª—è –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏
@Configuration
class ParallelConsumerConfig {
    
    @Bean
    fun kafkaListenerContainerFactory(): ConcurrentKafkaListenerContainerFactory<String, OrderEvent> {
        val factory = ConcurrentKafkaListenerContainerFactory<String, OrderEvent>()
        factory.consumerFactory = consumerFactory()
        
        // 10 –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã—Ö Consumer'–æ–≤
        factory.setConcurrency(10)
        
        return factory
    }
}

// 4. Reactive –ø–æ–¥—Ö–æ–¥ (Spring Kafka + Project Reactor)
@Service
class ReactiveKafkaConsumer(
    private val receiverOptions: ReceiverOptions<String, OrderEvent>
) {
    
    fun startConsuming() {
        KafkaReceiver.create(receiverOptions)
            .receive()
            .flatMap({ record ->
                // –ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ —Å backpressure
                processOrderAsync(record.value())
                    .doOnSuccess { record.receiverOffset().acknowledge() }
                    .onErrorResume { error ->
                        logger.error("Failed to process", error)
                        Mono.empty()
                    }
            }, 10)  // Concurrency = 10
            .subscribe()
    }
    
    private fun processOrderAsync(order: OrderEvent): Mono<Void> {
        return Mono.fromCallable {
            // –¢—è–∂—ë–ª–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞
            processOrder(order)
        }.subscribeOn(Schedulers.boundedElastic())
            .then()
    }
}
```

### –ö–ï–ô–° #8 | –£—Ä–æ–≤–µ–Ω—å: Middle
**–í–û–ü–†–û–°:** –ß—Ç–æ —Ç–∞–∫–æ–µ auto.offset.reset? –í —á—ë–º —Ä–∞–∑–Ω–∏—Ü–∞ –º–µ–∂–¥—É earliest –∏ latest?

**–û–¢–í–ï–¢:**
```kotlin
// auto.offset.reset: —á—Ç–æ –¥–µ–ª–∞—Ç—å, –µ—Å–ª–∏ offset –æ—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç –≤ –≥—Ä—É–ø–ø–µ
// (–Ω–æ–≤–∞—è –≥—Ä—É–ø–ø–∞ –∏–ª–∏ offset —É–¥–∞–ª—ë–Ω –∏–∑-–∑–∞ retention)

// earliest: –Ω–∞—á–∞—Ç—å —Å —Å–∞–º–æ–≥–æ –Ω–∞—á–∞–ª–∞ —Ç–æ–ø–∏–∫–∞
val propsEarliest = Properties().apply {
    put(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG, "earliest")
}
// –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ: –≥–∞—Ä–∞–Ω—Ç–∏—è —á—Ç–æ –Ω–µ –ø—Ä–æ–ø—É—Å—Ç–∏–º —Å–æ–æ–±—â–µ–Ω–∏—è (–Ω–∞–ø—Ä–∏–º–µ—Ä, audit log)

// latest: –Ω–∞—á–∞—Ç—å —Å –Ω–æ–≤—ã—Ö —Å–æ–æ–±—â–µ–Ω–∏–π (default)
val propsLatest = Properties().apply {
    put(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG, "latest")
}
// –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ: –Ω–∞–º –Ω—É–∂–Ω—ã —Ç–æ–ª—å–∫–æ —Å–≤–µ–∂–∏–µ –¥–∞–Ω–Ω—ã–µ (–Ω–∞–ø—Ä–∏–º–µ—Ä, real-time alerts)

// none: –±—Ä–æ—Å–∏—Ç—å –∏—Å–∫–ª—é—á–µ–Ω–∏–µ
val propsNone = Properties().apply {
    put(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG, "none")
}
// –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ: –∫—Ä–∏—Ç–∏—á–Ω—ã–µ —Å–∏—Å—Ç–µ–º—ã, –≥–¥–µ –ø—Ä–æ–ø—É—Å–∫ —Å–æ–æ–±—â–µ–Ω–∏–π –Ω–µ–¥–æ–ø—É—Å—Ç–∏–º

// –ü—Ä–∏–º–µ—Ä: –∞—É–¥–∏—Ç + –∞–ª–µ—Ä—Ç—ã
@Configuration
class MultiConsumerConfig {
    
    // –ê—É–¥–∏—Ç: —á–∏—Ç–∞–µ–º –í–°–Å
    @Bean("auditConsumerFactory")
    fun auditConsumerFactory(): ConsumerFactory<String, AuditEvent> {
        return DefaultKafkaConsumerFactory(mapOf(
            ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG to "localhost:9092",
            ConsumerConfig.GROUP_ID_CONFIG to "audit-consumer",
            ConsumerConfig.AUTO_OFFSET_RESET_CONFIG to "earliest"  // –° –Ω–∞—á–∞–ª–∞
        ))
    }
    
    // –ê–ª–µ—Ä—Ç—ã: —Ç–æ–ª—å–∫–æ –Ω–æ–≤—ã–µ
    @Bean("alertConsumerFactory")
    fun alertConsumerFactory(): ConsumerFactory<String, AlertEvent> {
        return DefaultKafkaConsumerFactory(mapOf(
            ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG to "localhost:9092",
            ConsumerConfig.GROUP_ID_CONFIG to "alert-consumer",
            ConsumerConfig.AUTO_OFFSET_RESET_CONFIG to "latest"  // –¢–æ–ª—å–∫–æ –Ω–æ–≤—ã–µ
        ))
    }
}

@Service
class AuditConsumer {
    
    @KafkaListener(
        topics = ["audit-events"],
        containerFactory = "auditConsumerFactory"
    )
    fun consumeAudit(event: AuditEvent) {
        // –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –≤—Å–µ —Å–æ–±—ã—Ç–∏—è —Å —Å–∞–º–æ–≥–æ –Ω–∞—á–∞–ª–∞
        auditRepository.save(event)
    }
}

@Service
class AlertConsumer {
    
    @KafkaListener(
        topics = ["alerts"],
        containerFactory = "alertConsumerFactory"
    )
    fun consumeAlert(event: AlertEvent) {
        // –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —Ç–æ–ª—å–∫–æ –Ω–æ–≤—ã–µ –∞–ª–µ—Ä—Ç—ã
        notificationService.send(event)
    }
}
```

### –ö–ï–ô–° #9 | –£—Ä–æ–≤–µ–Ω—å: Senior
**–í–û–ü–†–û–°:** –ö–∞–∫ —Ä–µ–∞–ª–∏–∑–æ–≤–∞—Ç—å Exactly-Once Semantics –≤ Kafka Consumer?

**–û–¢–í–ï–¢:**
```kotlin
// Exactly-Once: —Å–æ–æ–±—â–µ–Ω–∏–µ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–æ —Ä–æ–≤–Ω–æ –æ–¥–∏–Ω —Ä–∞–∑
// –ö–æ–º–±–∏–Ω–∞—Ü–∏—è: –∏–¥–µ–º–ø–æ—Ç–µ–Ω—Ç–Ω—ã–π Producer + —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏–∏ + –ø—Ä–∞–≤–∏–ª—å–Ω—ã–π Consumer

// 1. Producer —Å —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏—è–º–∏
@Configuration
class ExactlyOnceProducerConfig {
    
    @Bean
    fun producerFactory(): ProducerFactory<String, OrderEvent> {
        return DefaultKafkaProducerFactory(mapOf(
            ProducerConfig.BOOTSTRAP_SERVERS_CONFIG to "localhost:9092",
            ProducerConfig.ENABLE_IDEMPOTENCE_CONFIG to true,
            ProducerConfig.TRANSACTIONAL_ID_CONFIG to "order-producer-${UUID.randomUUID()}"
        ))
    }
    
    @Bean
    fun kafkaTransactionManager(): KafkaTransactionManager<String, OrderEvent> {
        return KafkaTransactionManager(producerFactory())
    }
}

// 2. Consumer —Å isolation.level=read_committed
@Configuration
class ExactlyOnceConsumerConfig {
    
    @Bean
    fun consumerFactory(): ConsumerFactory<String, OrderEvent> {
        return DefaultKafkaConsumerFactory(mapOf(
            ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG to "localhost:9092",
            ConsumerConfig.GROUP_ID_CONFIG to "order-processor",
            ConsumerConfig.ENABLE_AUTO_COMMIT_CONFIG to false,
            ConsumerConfig.ISOLATION_LEVEL_CONFIG to "read_committed"  // –¢–æ–ª—å–∫–æ committed
        ))
    }
}

// 3. –¢—Ä–∞–Ω–∑–∞–∫—Ü–∏–æ–Ω–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞: Kafka ‚Üí DB ‚Üí Kafka
@Service
class ExactlyOnceOrderProcessor(
    private val kafkaTemplate: KafkaTemplate<String, OrderEvent>,
    private val orderRepository: OrderRepository
) {
    
    @Transactional("kafkaTransactionManager")
    @KafkaListener(topics = ["orders"])
    fun processOrder(orderEvent: OrderEvent, acknowledgment: Acknowledgment) {
        // –í—Å—ë –≤ —Ä–∞–º–∫–∞—Ö –æ–¥–Ω–æ–π —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏–∏:
        // 1. –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ DB
        val order = orderRepository.save(orderEvent.toEntity())
        
        // 2. –û—Ç–ø—Ä–∞–≤–ª—è–µ–º –≤ –¥—Ä—É–≥–æ–π —Ç–æ–ø–∏–∫
        kafkaTemplate.send("order-processed", order.id.toString(), OrderProcessedEvent(order))
        
        // 3. Commit offset
        acknowledgment.acknowledge()
        
        // –ï—Å–ª–∏ —á—Ç–æ-—Ç–æ —É–ø–∞–¥—ë—Ç ‚Äî –æ—Ç–∫–∞—Ç—è—Ç—Å—è –í–°–ï –∏–∑–º–µ–Ω–µ–Ω–∏—è (DB + Kafka)
    }
}

// 4. –ò–¥–µ–º–ø–æ—Ç–µ–Ω—Ç–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ (–Ω–∞ —Å–ª—É—á–∞–π –¥—É–±–ª–∏–∫–∞—Ç–æ–≤)
@Service
class IdempotentOrderProcessor(
    private val orderRepository: OrderRepository,
    private val processedEventsRepository: ProcessedEventRepository
) {
    
    @KafkaListener(topics = ["orders"])
    @Transactional
    fun processOrder(
        orderEvent: OrderEvent,
        @Header(KafkaHeaders.OFFSET) offset: Long,
        @Header(KafkaHeaders.RECEIVED_PARTITION_ID) partition: Int
    ) {
        val eventId = "orders-$partition-$offset"
        
        // –ü—Ä–æ–≤–µ—Ä—è–µ–º, –Ω–µ –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–ª–∏ –ª–∏ —É–∂–µ
        if (processedEventsRepository.existsById(eventId)) {
            logger.info("Event $eventId already processed, skipping")
            return
        }
        
        // –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º
        val order = orderRepository.save(orderEvent.toEntity())
        
        // –ü–æ–º–µ—á–∞–µ–º –∫–∞–∫ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω–æ–µ
        processedEventsRepository.save(ProcessedEvent(eventId, Instant.now()))
        
        logger.info("Processed order ${order.id}")
    }
}

// 5. Transactional Outbox –¥–ª—è –≥–∞—Ä–∞–Ω—Ç–∏—Ä–æ–≤–∞–Ω–Ω–æ–π –¥–æ—Å—Ç–∞–≤–∫–∏
@Entity
data class OutboxEvent(
    @Id val id: UUID = UUID.randomUUID(),
    val topic: String,
    val key: String,
    val payload: String,
    var status: OutboxStatus = OutboxStatus.PENDING,
    val createdAt: Instant = Instant.now()
)

@Service
class TransactionalOutboxService(
    private val outboxRepository: OutboxRepository,
    private val kafkaTemplate: KafkaTemplate<String, String>
) {
    
    @Transactional
    fun saveOrderAndScheduleEvent(order: Order) {
        // 1. –°–æ—Ö—Ä–∞–Ω—è–µ–º –∑–∞–∫–∞–∑
        orderRepository.save(order)
        
        // 2. –°–æ—Ö—Ä–∞–Ω—è–µ–º —Å–æ–±—ã—Ç–∏–µ –≤ Outbox (–≤ —Ç–æ–π –∂–µ —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏–∏)
        outboxRepository.save(
            OutboxEvent(
                topic = "orders",
                key = order.id.toString(),
                payload = objectMapper.writeValueAsString(order)
            )
        )
        // –ï—Å–ª–∏ —É–ø–∞–¥—ë—Ç ‚Äî –æ—Ç–∫–∞—Ç–∏—Ç—Å—è –∏ –∑–∞–∫–∞–∑, –∏ —Å–æ–±—ã—Ç–∏–µ
    }
    
    @Scheduled(fixedDelay = 1000)
    fun publishPendingEvents() {
        val pending = outboxRepository.findTop100ByStatusOrderByCreatedAt(OutboxStatus.PENDING)
        
        pending.forEach { event ->
            try {
                kafkaTemplate.send(event.topic, event.key, event.payload).get()
                
                event.status = OutboxStatus.PUBLISHED
                outboxRepository.save(event)
            } catch (e: Exception) {
                logger.error("Failed to publish event ${event.id}", e)
            }
        }
    }
}
```

## DLQ (Dead Letter Queue)

### –ö–ï–ô–° #10 | –£—Ä–æ–≤–µ–Ω—å: Middle
**–í–û–ü–†–û–°:** –ö–∞–∫ —Ä–µ–∞–ª–∏–∑–æ–≤–∞—Ç—å Dead Letter Queue –¥–ª—è —Å–æ–æ–±—â–µ–Ω–∏–π, –∫–æ—Ç–æ—Ä—ã–µ –Ω–µ —É–¥–∞–ª–æ—Å—å –æ–±—Ä–∞–±–æ—Ç–∞—Ç—å –ø–æ—Å–ª–µ N –ø–æ–ø—ã—Ç–æ–∫?

**–û–¢–í–ï–¢:**
```kotlin
@Service
class OrderConsumerWithDLQ(
    private val kafkaTemplate: KafkaTemplate<String, String>
) {
    
    @KafkaListener(topics = ["orders"], groupId = "order-processing")
    fun consumeOrder(
        message: String,
        @Header(KafkaHeaders.RECEIVED_PARTITION_ID) partition: Int,
        @Header(KafkaHeaders.OFFSET) offset: Long
    ) {
        var attempts = 0
        val maxAttempts = 3
        
        while (attempts < maxAttempts) {
            try {
                processOrder(message)
                return  // –£—Å–ø–µ—à–Ω–æ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–æ
            } catch (e: Exception) {
                attempts++
                logger.warn("Failed attempt $attempts/$maxAttempts", e)
                
                if (attempts < maxAttempts) {
                    Thread.sleep(1000 * attempts)  // Exponential backoff
                }
            }
        }
        
        // –ü–æ—Å–ª–µ 3 –ø–æ–ø—ã—Ç–æ–∫ ‚Üí DLQ
        logger.error("Sending to DLQ after $maxAttempts attempts")
        kafkaTemplate.send(
            "orders-dlq",
            DLQMessage(
                originalMessage = message,
                partition = partition,
                offset = offset,
                attempts = attempts,
                lastError = "Processing failed",
                timestamp = System.currentTimeMillis()
            )
        )
    }
    
    private fun processOrder(message: String) {
        // –ë–∏–∑–Ω–µ—Å-–ª–æ–≥–∏–∫–∞
    }
}

// Consumer –¥–ª—è DLQ (—Ä—É—á–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞)
@KafkaListener(topics = ["orders-dlq"], groupId = "dlq-handler")
fun handleDLQ(dlqMessage: DLQMessage) {
    logger.error("DLQ message: ${dlqMessage.originalMessage}")
    
    // –û–ø—Ü–∏–∏:
    // 1. –ê–ª–µ—Ä—Ç –≤ –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥
    alertService.sendAlert("DLQ message received", dlqMessage)
    
    // 2. –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤ –ë–î –¥–ª—è —Ä—É—á–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏
    dlqRepository.save(dlqMessage)
    
    // 3. –ü–æ–ø—ã—Ç–∫–∞ –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–π –∫–æ—Ä—Ä–µ–∫—Ç–∏—Ä–æ–≤–∫–∏ –¥–∞–Ω–Ω—ã—Ö
    // val corrected = tryCorrect(dlqMessage.originalMessage)
    // if (corrected != null) kafkaTemplate.send("orders", corrected)
}
```

## Schema Registry

### –ö–ï–ô–° #11 | –£—Ä–æ–≤–µ–Ω—å: Senior
**–í–û–ü–†–û–°:** –ó–∞—á–µ–º –Ω—É–∂–µ–Ω Schema Registry? –ö–∞–∫ —Ä–∞–±–æ—Ç–∞—Ç—å —Å Avro –≤ Kafka?

**–û–¢–í–ï–¢:**
```kotlin
// Schema Registry: —Ü–µ–Ω—Ç—Ä–∞–ª–∏–∑–æ–≤–∞–Ω–Ω–æ–µ —Ö—Ä–∞–Ω–µ–Ω–∏–µ —Å—Ö–µ–º –¥–∞–Ω–Ω—ã—Ö
// –ü—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞:
// - –ö–æ–Ω—Ç—Ä–æ–ª—å —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏ (backward, forward, full)
// - –ö–æ–º–ø–∞–∫—Ç–Ω–∞—è —Å–µ—Ä–∏–∞–ª–∏–∑–∞—Ü–∏—è (binary)
// - –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∞—è –≤–∞–ª–∏–¥–∞—Ü–∏—è

// 1. Avro —Å—Ö–µ–º–∞ (order.avsc)
"""
{
  "type": "record",
  "name": "Order",
  "namespace": "com.example.events",
  "fields": [
    {"name": "id", "type": "string"},
    {"name": "customerId", "type": "string"},
    {"name": "amount", "type": "double"},
    {"name": "status", "type": {
      "type": "enum",
      "name": "OrderStatus",
      "symbols": ["CREATED", "PAID", "SHIPPED", "DELIVERED"]
    }}
  ]
}
"""

// 2. Producer –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è
@Configuration
class AvroProducerConfig {
    
    @Bean
    fun producerFactory(): ProducerFactory<String, Order> {
        return DefaultKafkaProducerFactory(mapOf(
            ProducerConfig.BOOTSTRAP_SERVERS_CONFIG to "localhost:9092",
            ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG to StringSerializer::class.java,
            
            // Avro serializer
            ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG to KafkaAvroSerializer::class.java,
            "schema.registry.url" to "http://localhost:8081"
        ))
    }
}

@Service
class OrderProducer(private val kafkaTemplate: KafkaTemplate<String, Order>) {
    
    fun sendOrder(order: Order) {
        // Serializer –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏:
        // 1. –†–µ–≥–∏—Å—Ç—Ä–∏—Ä—É–µ—Ç —Å—Ö–µ–º—É –≤ Registry (–µ—Å–ª–∏ –Ω–æ–≤–∞—è)
        // 2. –°–µ—Ä–∏–∞–ª–∏–∑—É–µ—Ç –≤ binary Avro
        // 3. –î–æ–±–∞–≤–ª—è–µ—Ç schema ID –≤ –Ω–∞—á–∞–ª–æ —Å–æ–æ–±—â–µ–Ω–∏—è
        kafkaTemplate.send("orders", order.id, order)
    }
}

// 3. Consumer –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è
@Configuration
class AvroConsumerConfig {
    
    @Bean
    fun consumerFactory(): ConsumerFactory<String, Order> {
        return DefaultKafkaConsumerFactory(mapOf(
            ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG to "localhost:9092",
            ConsumerConfig.GROUP_ID_CONFIG to "order-processor",
            ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG to StringDeserializer::class.java,
            
            // Avro deserializer
            ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG to KafkaAvroDeserializer::class.java,
            "schema.registry.url" to "http://localhost:8081",
            "specific.avro.reader" to true  // –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –∫–ª–∞—Å—Å—ã
        ))
    }
}

@Service
class OrderConsumer {
    
    @KafkaListener(topics = ["orders"])
    fun consume(order: Order) {
        // Deserializer –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏:
        // 1. –ß–∏—Ç–∞–µ—Ç schema ID –∏–∑ —Å–æ–æ–±—â–µ–Ω–∏—è
        // 2. –ü–æ–ª—É—á–∞–µ—Ç —Å—Ö–µ–º—É –∏–∑ Registry
        // 3. –î–µ—Å–µ—Ä–∏–∞–ª–∏–∑—É–µ—Ç –≤ –æ–±—ä–µ–∫—Ç Order
        
        logger.info("Received order: ${order.id}, status: ${order.status}")
    }
}

// 4. –≠–≤–æ–ª—é—Ü–∏—è —Å—Ö–µ–º—ã (–¥–æ–±–∞–≤–ª–µ–Ω–∏–µ –ø–æ–ª—è)
// order-v2.avsc
"""
{
  "type": "record",
  "name": "Order",
  "namespace": "com.example.events",
  "fields": [
    {"name": "id", "type": "string"},
    {"name": "customerId", "type": "string"},
    {"name": "amount", "type": "double"},
    {"name": "status", "type": "OrderStatus"},
    {"name": "discount", "type": ["null", "double"], "default": null}  // –ù–æ–≤–æ–µ –ø–æ–ª–µ
  ]
}
"""

// –¢–∏–ø—ã —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏:
// BACKWARD (default): –Ω–æ–≤—ã–π Consumer –º–æ–∂–µ—Ç —á–∏—Ç–∞—Ç—å —Å—Ç–∞—Ä—ã–µ —Å–æ–æ–±—â–µ–Ω–∏—è
// FORWARD: —Å—Ç–∞—Ä—ã–π Consumer –º–æ–∂–µ—Ç —á–∏—Ç–∞—Ç—å –Ω–æ–≤—ã–µ —Å–æ–æ–±—â–µ–Ω–∏—è
// FULL: –∏ —Ç–æ, –∏ –¥—Ä—É–≥–æ–µ
// NONE: –±–µ–∑ –ø—Ä–æ–≤–µ—Ä–∫–∏

// –ù–∞—Å—Ç—Ä–æ–π–∫–∞ —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏
@Bean
fun schemaRegistryClient(): SchemaRegistryClient {
    return CachedSchemaRegistryClient("http://localhost:8081", 100).apply {
        updateCompatibility("orders-value", "BACKWARD")
    }
}
```

## Kafka Streams

### –ö–ï–ô–° #12 | –£—Ä–æ–≤–µ–Ω—å: Senior
**–í–û–ü–†–û–°:** –ö–∞–∫ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å Kafka Streams –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Å–æ–±—ã—Ç–∏–π –≤ real-time?

**–û–¢–í–ï–¢:**
```kotlin
// Kafka Streams: –±–∏–±–ª–∏–æ—Ç–µ–∫–∞ –¥–ª—è stream processing
// –ü—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞: stateful processing, exactly-once, fault-tolerant

@Configuration
@EnableKafkaStreams
class KafkaStreamsConfig {
    
    @Bean
    fun kStreamsConfig(): KafkaStreamsConfiguration {
        return KafkaStreamsConfiguration(mapOf(
            StreamsConfig.APPLICATION_ID_CONFIG to "order-analytics",
            StreamsConfig.BOOTSTRAP_SERVERS_CONFIG to "localhost:9092",
            StreamsConfig.DEFAULT_KEY_SERDE_CLASS_CONFIG to Serdes.String()::class.java,
            StreamsConfig.DEFAULT_VALUE_SERDE_CLASS_CONFIG to JsonSerde::class.java,
            
            // Exactly-once
            StreamsConfig.PROCESSING_GUARANTEE_CONFIG to StreamsConfig.EXACTLY_ONCE_V2
        ))
    }
}

// –ü—Ä–∏–º–µ—Ä 1: –ê–≥—Ä–µ–≥–∞—Ü–∏—è –∑–∞–∫–∞–∑–æ–≤ –ø–æ —Å—Ç–∞—Ç—É—Å—É
@Component
class OrderStatusAggregator {
    
    @Bean
    fun orderStatusStream(streamsBuilder: StreamsBuilder): KStream<String, Order> {
        val orders: KStream<String, Order> = streamsBuilder.stream("orders")
        
        // –ì—Ä—É–ø–ø–∏—Ä–æ–≤–∫–∞ –ø–æ —Å—Ç–∞—Ç—É—Å—É + –ø–æ–¥—Å—á—ë—Ç
        orders
            .groupBy({ key, order -> order.status.name }, Grouped.with(Serdes.String(), orderSerde))
            .count(Materialized.as("order-count-by-status"))
            .toStream()
            .to("order-statistics", Produced.with(Serdes.String(), Serdes.Long()))
        
        return orders
    }
}

// –ü—Ä–∏–º–µ—Ä 2: Joining –∑–∞–∫–∞–∑–æ–≤ –∏ –ø–ª–∞—Ç–µ–∂–µ–π
@Component
class OrderPaymentJoiner {
    
    @Bean
    fun orderPaymentStream(streamsBuilder: StreamsBuilder): KStream<String, OrderWithPayment> {
        val orders: KStream<String, Order> = streamsBuilder.stream("orders")
        val payments: KTable<String, Payment> = streamsBuilder.table("payments")
        
        // Join –ø–æ orderId
        return orders.join(
            payments,
            { order, payment -> OrderWithPayment(order, payment) },
            Joined.with(Serdes.String(), orderSerde, paymentSerde)
        ).to("orders-with-payments")
    }
}

// –ü—Ä–∏–º–µ—Ä 3: Windowed aggregation (–∑–∞–∫–∞–∑—ã –∑–∞ –ø–æ—Å–ª–µ–¥–Ω–∏–π —á–∞—Å)
@Component
class HourlyOrderAnalytics {
    
    @Bean
    fun hourlyRevenue(streamsBuilder: StreamsBuilder): KStream<String, Order> {
        val orders: KStream<String, Order> = streamsBuilder.stream("orders")
        
        orders
            .groupByKey()
            .windowedBy(TimeWindows.ofSizeWithNoGrace(Duration.ofHours(1)))
            .aggregate(
                { 0.0 },
                { key, order, total -> total + order.amount },
                Materialized.with(Serdes.String(), Serdes.Double())
            )
            .toStream()
            .map { windowedKey, total ->
                KeyValue(
                    windowedKey.key(),
                    RevenueReport(
                        period = windowedKey.window().startTime(),
                        total = total
                    )
                )
            }
            .to("hourly-revenue")
        
        return orders
    }
}

// –ü—Ä–∏–º–µ—Ä 4: –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è + —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü–∏—è
@Component
class LargeOrderProcessor {
    
    @Bean
    fun largeOrdersStream(streamsBuilder: StreamsBuilder): KStream<String, Order> {
        return streamsBuilder
            .stream<String, Order>("orders")
            .filter { key, order -> order.amount > 10000 }
            .mapValues { order ->
                LargeOrderAlert(
                    orderId = order.id,
                    amount = order.amount,
                    customerId = order.customerId,
                    timestamp = Instant.now()
                )
            }
            .to("large-order-alerts")
    }
}

// Interactive Queries: —á—Ç–µ–Ω–∏–µ state store
@RestController
@RequestMapping("/api/analytics")
class AnalyticsController(
    private val kafkaStreams: KafkaStreams
) {
    
    @GetMapping("/order-count/{status}")
    fun getOrderCountByStatus(@PathVariable status: String): Long {
        val store: ReadOnlyKeyValueStore<String, Long> = 
            kafkaStreams.store(
                StoreQueryParameters.fromNameAndType(
                    "order-count-by-status",
                    QueryableStoreTypes.keyValueStore()
                )
            )
        
        return store.get(status) ?: 0L
    }
}
```

## Performance & Monitoring

### –ö–ï–ô–° #13 | –£—Ä–æ–≤–µ–Ω—å: Middle
**–í–û–ü–†–û–°:** –ö–∞–∫ –º–æ–Ω–∏—Ç–æ—Ä–∏—Ç—å Kafka Producer/Consumer? –ö–∞–∫–∏–µ –º–µ—Ç—Ä–∏–∫–∏ –≤–∞–∂–Ω—ã?

**–û–¢–í–ï–¢:**
```kotlin
// –ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ —á–µ—Ä–µ–∑ Micrometer (Spring Boot Actuator)

@Configuration
class KafkaMonitoringConfig {
    
    @Bean
    fun kafkaMetricsProducer(meterRegistry: MeterRegistry): ProducerFactory<String, OrderEvent> {
        val factory = DefaultKafkaProducerFactory<String, OrderEvent>(producerProps())
        
        // –î–æ–±–∞–≤–ª—è–µ–º –º–µ—Ç—Ä–∏–∫–∏ Producer
        factory.addListener(MicrometerProducerListener(meterRegistry))
        
        return factory
    }
    
    @Bean
    fun kafkaMetricsConsumer(meterRegistry: MeterRegistry): ConsumerFactory<String, OrderEvent> {
        val factory = DefaultKafkaConsumerFactory<String, OrderEvent>(consumerProps())
        
        // –î–æ–±–∞–≤–ª—è–µ–º –º–µ—Ç—Ä–∏–∫–∏ Consumer
        factory.addListener(MicrometerConsumerListener(meterRegistry))
        
        return factory
    }
}

// –ö–ª—é—á–µ–≤—ã–µ –º–µ—Ç—Ä–∏–∫–∏ Producer:
// - record-send-rate: —Å–æ–æ–±—â–µ–Ω–∏–π/—Å–µ–∫
// - record-error-rate: –æ—à–∏–±–æ–∫/—Å–µ–∫
// - request-latency-avg: —Å—Ä–µ–¥–Ω—è—è –∑–∞–¥–µ—Ä–∂–∫–∞
// - buffer-available-bytes: –¥–æ—Å—Ç—É–ø–Ω–∞—è –ø–∞–º—è—Ç—å –≤ –±—É—Ñ–µ—Ä–µ

// –ö–ª—é—á–µ–≤—ã–µ –º–µ—Ç—Ä–∏–∫–∏ Consumer:
// - records-consumed-rate: —Å–æ–æ–±—â–µ–Ω–∏–π/—Å–µ–∫
// - records-lag-max: –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –æ—Ç—Å—Ç–∞–≤–∞–Ω–∏–µ
// - fetch-latency-avg: —Å—Ä–µ–¥–Ω—è—è –∑–∞–¥–µ—Ä–∂–∫–∞ fetch
// - commit-latency-avg: —Å—Ä–µ–¥–Ω—è—è –∑–∞–¥–µ—Ä–∂–∫–∞ commit

@Service
class KafkaHealthIndicator(
    private val kafkaTemplate: KafkaTemplate<String, String>,
    private val listenerContainerRegistry: KafkaListenerEndpointRegistry
) : HealthIndicator {
    
    override fun health(): Health {
        // –ü—Ä–æ–≤–µ—Ä—è–µ–º –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—å Producer
        val producerHealthy = try {
            kafkaTemplate.send("health-check", "ping").get(5, TimeUnit.SECONDS)
            true
        } catch (e: Exception) {
            false
        }
        
        // –ü—Ä–æ–≤–µ—Ä—è–µ–º Consumer
        val consumerHealthy = listenerContainerRegistry.listenerContainers
            .all { it.isRunning }
        
        return if (producerHealthy && consumerHealthy) {
            Health.up()
                .withDetail("producer", "UP")
                .withDetail("consumer", "UP")
                .build()
        } else {
            Health.down()
                .withDetail("producer", if (producerHealthy) "UP" else "DOWN")
                .withDetail("consumer", if (consumerHealthy) "UP" else "DOWN")
                .build()
        }
    }
}

// –ö–∞—Å—Ç–æ–º–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏
@Service
class OrderConsumerWithMetrics(
    private val meterRegistry: MeterRegistry
) {
    
    private val processedCounter = meterRegistry.counter("orders.processed")
    private val failedCounter = meterRegistry.counter("orders.failed")
    private val processingTimer = meterRegistry.timer("orders.processing.time")
    
    @KafkaListener(topics = ["orders"])
    fun consume(order: OrderEvent) {
        processingTimer.recordCallable {
            try {
                processOrder(order)
                processedCounter.increment()
            } catch (e: Exception) {
                failedCounter.increment()
                throw e
            }
        }
    }
}

// –ê–ª–µ—Ä—Ç—ã –Ω–∞ –æ—Å–Ω–æ–≤–µ –º–µ—Ç—Ä–∏–∫
@Component
class ConsumerLagAlert(
    private val meterRegistry: MeterRegistry,
    private val alertService: AlertService
) {
    
    @Scheduled(fixedDelay = 60000)  // –ö–∞–∂–¥—É—é –º–∏–Ω—É—Ç—É
    fun checkConsumerLag() {
        val lag = meterRegistry.get("kafka.consumer.records.lag.max")
            .tag("topic", "orders")
            .gauge()
            .value()
        
        if (lag > 10000) {
            alertService.send("High consumer lag detected: $lag messages")
        }
    }
}
```

---

üìä **–ú–æ–¥–µ–ª—å**: Claude Sonnet 4.5 | **–ö–µ–π—Å–æ–≤**: 25 | **–°—Ç–æ–∏–º–æ—Å—Ç—å**: ~$1.20

*–í–µ—Ä—Å–∏—è: 2.0 | –Ø–Ω–≤–∞—Ä—å 2026*

